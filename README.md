# dynasafe-interview
this dynasafe interview repo  

## architecture
![alt text](architecture.drawio.png)

## create cluster

1. [install docker](https://docs.docker.com/engine/install/ubuntu/), for kind  
[install mise](https://mise.jdx.dev/getting-started.html#installing-mise-cli) for runtime manage  

2. install runtime  
config in [mise.toml](mise.toml)
use to install kubectl,helm,kind
```bash
mise install
```

3. create kind cluster  
kind config in [kind/kind-config.yaml](kind/kind-config.yaml)  
in config file define 4 node, all use kubernetes 1.32.5  
- 1 control-plane node 
- 1 work node(infra)  
- 2 work node(application)  

work node add label `role` as node group  
infra node set `extraPortMappings` for docker forward port to work node  

crate cluster by config  
```bash
$ kind create cluster --config kind/kind-config.yaml
Creating cluster "kind" ...
 âœ“ Ensuring node image (kindest/node:v1.32.5) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦ ğŸ“¦ ğŸ“¦ ğŸ“¦ ğŸ“¦ ğŸ“¦ ğŸ“¦  
 âœ“ Configuring the external load balancer âš– 
 âœ“ Writing configuration ğŸ“œ 
 âœ“ Starting control-plane ğŸ•¹ 
 âœ“ Installing CNI ğŸ”Œ 
 âœ“ Installing StorageClass ğŸ’¾ 
 âœ“ Joining more control-plane nodes ğŸ® 
 âœ“ Joining worker nodes ğŸšœ 
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Thanks for using kind! ğŸ˜Š
```

4. connect to cluster
export kubeconfig and setup kubectl  
```bash
kind get kubeconfig > kind/kubeconfig.yaml

source <(kubectl completion bash)
complete -o default -F __start_kubectl k
alias k="kubectl"
```

test  
```bash
$ k version
Client Version: v1.32.7
Kustomize Version: v5.5.0
Server Version: v1.32.5
```

5. install metallb  
it's use to implement service type loadbalance  

check infra node ip(INTERNAL-IP)  
```bash
$ k get node --selector='role=infra' -o wide 
NAME          STATUS   ROLES    AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION    CONTAINER-RUNTIME
kind-worker   Ready    <none>   20m   v1.32.5   192.168.252.194   <none>        Debian GNU/Linux 12 (bookworm)   6.14.0-1007-oem   containerd://2.1.1
```

check [charts/metallb/IPAddressPool.yaml](charts/metallb/IPAddressPool.yaml) .spec.addresses is setup infra node ip  
if not, update first  
in helm values file [charts/metallb/values.yaml](charts/metallb/values.yaml) set `nodeAffinity` to set speaker is running in infra node  
and config L2 mode  

```bash
helm repo add metallb https://metallb.github.io/metallb
helm install metallb metallb/metallb \
  --version 0.15.2 \
  -f charts/metallb/values.yaml \
  --create-namespace \
  --namespace metallb-system

k apply -f charts/metallb/IPAddressPool.yaml
k apply -f charts/metallb/L2Advertisement.yaml
```

6. install haproxy-ingress  
use haproxy to support ingress feature  
helm value file [charts/haproxy-ingress/values.yaml](charts/haproxy-ingress/values.yaml) setup  
- running in infra node  
- use service type loadbalance to expose ingress controller  
- monitor by prometheus  

```bash
helm repo add haproxytech https://haproxytech.github.io/helm-charts

helm upgrade --install haproxy-kubernetes-ingress haproxytech/kubernetes-ingress \
  --version 1.44.5 \
  -f charts/haproxy-ingress/values.yaml \
  --create-namespace \
  --namespace haproxy-controller
```

## install monitor  
1. install metrics-server for HPA  
helm value file [charts/metrics-server/values.yaml](charts/metrics-server/values.yaml) setup  
- running in infra node  
- ignore tls verify certificate  

```bash
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm upgrade --install metrics-server metrics-server/metrics-server \
  --version 3.12.2 \
  -f charts/metrics-server/values.yaml \
  --create-namespace \
  --namespace kube-system
```

2. install prometheus,node-exporter,kube-state-metrics  
helm value file [charts/prometheus/values.yaml](charts/prometheus/values.yaml) setup  
- running in infra node  
- disable alertmanager (not used)  
- disable prometheus-pushgateway (not used)  
- expose by ingress  

```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm upgrade --install prometheus prometheus-community/prometheus \
  --version 27.28.0 \
  -f charts/prometheus/values.yaml \
  --create-namespace \
  --namespace prometheus
```

test  
```bash
$ curl --resolve prometheus.example.local:80:127.0.0.1 http://prometheus.example.local/-/healthy
Prometheus Server is Healthy.
```

3. monitor etcd  
because etcd cannot configure `listen-metrics-urls` other than 127.0.0.1:2381  
here install a reverse proxy to scrape metric  
etcd use `hostNetwork`, so reverse proxy use `hostNetwork` too can scrape metrics from 127.0.0.1  
```bash
helm upgrade --install etcd-proxy charts/nginx \
  -f charts/nginx/etcd-metrics.yaml \
  --namespace prometheus
```

4. start grafana   
use docker to running grafana, and use provisioning feature to config grafana datasource and dashboard   
```bash
docker compose -f grafana/docker-compose.yml up -d 
```

open http://127.0.0.1:3000 will see grafana is started  
![grafana](images/grafana.png)

## explain monitor  
dashboard save in 2 folder `public`,`custom`  

public: user shared dashboard  
custom: self-design dashboard  

éƒ¨ä»½ monitor å›  public dashboard æœ‰éå¸¸å®Œæ•´çš„ panel ä¸¦è§£é‡‹, å› æ­¤ç›´æ¥æ¡ç”¨ä¸å†å¦å¤–è£½ä½œ dashboard  
æœ‰ä¸æ»¿è¶³éœ€æ±‚æˆ–ç„¡ public dashboard çš„æ‰è‡ªè¡Œè£½ä½œ dashboard  

å› æ‰€æœ‰ dashbaord panel çœ¾å¤š, æˆ‘åªæ¡éƒ¨ä»½è§£é‡‹  

### node monitor  
**for detail node monitor**  
[node-exporter-full](http://127.0.0.1:3000/d/rYdddlPWk/node-exporter-full) (metric by 1 node)  
![node-exporter-full](images/node-exporter-full.png)  

example system Utilization, Saturation: disk space  
å·¦å´é¡¯ç¤ºå¯ç”¨ç©ºé–“,å³å´é¡¯ç¤ºå·²ä½¿ç”¨ç©ºé–“  
![alt text](images/node-exporter-full_space.png)  

example system error: OOM-Kill  
![alt text](images/node-exporter-oom.png)  


**for overview node monitor**  
[node-exporter-overview](http://127.0.0.1:3000/d/oJz6m6LVz/node-exporter-overview) (metric by multiple node)  
![node-exporter-overview](images/node-exporter-overview.png)  

è¨­è¨ˆç”¨é€” for é‡å°å¤šå€‹ node é€²è¡Œ monitor  
åªçœ‹ cpu/memory/network/disk summary, é¿å…å›  metric éå¤šé€ æˆä½¿ç”¨å›°é›£  

### kubernetes cluster monitor  
**cluster wide**  
[k8s-dashboard](http://127.0.0.1:3000/d/besllvgck2iv4f/k8s-dashboard)  
![](images/k8s_cluster.png)  

å° cluster overview çš„ monitor å¯ä»¥çœ‹åˆ° resource count  
ä»¥åŠç°¡æ˜“ pod çš„ monitor  

**container wide**  
[pod-monitor](http://127.0.0.1:3000/d/b1f2555d-fe63-43fe-b67e-940ade68fdea/pod-monitor)  
![pod](images/pod.png)

å° container çš„ monitor  

è¨­è¨ˆ select single pod æ™‚èƒ½é¡¯ç¤º limit  
![pod_limit](images/pod_limit.png)

monitor pod cpu throuttling  
ç›£æ§ Pod çš„ CPU è¢«é™åˆ¶çš„æƒ…æ³  
![pod_cpu_throuttling](images/pod_cpu_throuttling.png)

### etcd monitor  
[etcd-cluster-overview](http://127.0.0.1:3000/d/etcd_cluster/etcd-cluster-overview)
![etcd](images/etcd.png)

example monitor etcd issue  
![alt text](images/etcd_issue.png)  
heartbeat failures: è¨˜éŒ„ etcd å¢é›†ä¸­çš„ Leader ç¯€é»å‘ Follower ç¯€é»å‚³é€å¿ƒè·³è¨Šæ¯å¤±æ•—çš„ç¸½æ¬¡æ•¸ã€‚å¿ƒè·³è¨Šæ¯å°æ–¼ç¶­æŒ Leader çš„é ˜å°åœ°ä½ä»¥åŠè®“ Follower ç¯€é»ç¢ºèª Leader ä»ç„¶å­˜æ´»  
health failures: è¿½è¹¤ etcd æˆå“¡è‡ªèº«å¥åº·æª¢æŸ¥å¤±æ•—çš„ç¸½æ¬¡æ•¸  

Slow Applies: è¨˜éŒ„ etcd å¯«å…¥èŠ±è²»æ™‚é–“éé•·çš„æ¬¡æ•¸  
Slow Read Indexes: è¨˜éŒ„ etcd è®€å–èŠ±è²»æ™‚é–“éé•·çš„æ¬¡æ•¸  

### Prometheus   
[prometheus](http://127.0.0.1:3000/d/PROMETHEUS1/prometheus)
![prometheus](images/prometheus.png)


## demo application
use httpbin for demo  
helm value file [charts/httpbin/values.yaml](charts/httpbin/values.yaml) setup  
- running in infra node  
- expose by ingress
- HPA for maxpod 10, cpu use 50%

```bash
helm upgrade --install httpbin charts/httpbin
```

hpa object 
```bash
$ k get hpa
NAME      REFERENCE            TARGETS       MINPODS   MAXPODS   REPLICAS   AGE
httpbin   Deployment/httpbin   cpu: 0%/50%   1         10        1          98m

```

test
```bash
$ curl --resolve httpbin.chart-example.local:80:127.0.0.1 http://httpbin.chart-example.local/ip
{
  "origin": "10.244.1.1"
}
```

## teardown
```bash
docker compose -f grafana/docker-compose.yml down -v 
kind delete cluster -n kind
```